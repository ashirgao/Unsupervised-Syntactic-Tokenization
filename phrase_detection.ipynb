{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_PROCESSES = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read already downloaded data. (Find links to corpus below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://www.statmt.org/wmt11/training-monolingual-news-2011.tgz\n",
    "# http://www.statmt.org/wmt11/training-monolingual-news-2010.tgz\n",
    "corpus1 = open(\"data/training-monolingual/news.2011.en.shuffled\",\"r\").read()\n",
    "corpus2 = open(\"data/training-monolingual/news.2010.en.shuffled\",\"r\").read()\n",
    "corpus = corpus1 + \"\\n\" + corpus2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences : 20142184\n"
     ]
    }
   ],
   "source": [
    "sentences = corpus.split(\"\\n\")\n",
    "\n",
    "for i,v in enumerate(sentences):\n",
    "    sentences[i] = v.replace(\",\",\"\").replace(\".\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\"/\",\"\").replace(\"'\",\"\").lower()\n",
    "print(\"Total sentences : {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train CountVectorizer and generate dictionary (and also see how to generate a reverse one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary : 1050336\n"
     ]
    }
   ],
   "source": [
    "vectorizer = vectorizer.fit(sentences)\n",
    "print(\"Length of dictionary : {}\".format(len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = vectorizer.vocabulary_\n",
    "index2word = dict(zip(word2index.values(),word2index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generate a co-occurance matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unordered_co_mat = (sentences_mat.T * sentences_mat) \n",
    "# unordered_co_mat.setdiag(0) \n",
    "# print(\"Shape of co-occurance matrix (unordered) - {}\".format(unordered_co_mat.shape))\n",
    "# list(word2index.keys())[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function that generates position aware co-occurance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dist_map(dist):\n",
    "    \"\"\" Creates co-occurance matrix of size(vocab_len, vocal_len) between all elements\n",
    "        at given distance \"dist\" in corpus\n",
    "    \"\"\"\n",
    "    # initialize matrix to zeros\n",
    "    dist_mat = sparse.lil_matrix((len(word2index),len(word2index)),dtype=np.int32)\n",
    "    \n",
    "    # train on random fixed length subset of corpus\n",
    "#     for sentence in tqdm(random.sample(sentences,100000)):\n",
    "    # OR\n",
    "    # train on all corpus\n",
    "    for sentence in tqdm(sentences):\n",
    "\n",
    "        words = list(filter(lambda a: a != \"\", sentence.split(\" \")))\n",
    "        for i,v in enumerate(words[:-dist]): \n",
    "            try:\n",
    "                dist_mat[word2index[v],word2index[words[i+dist]]] += 1\n",
    "            except:\n",
    "                # fails if word not present in dictionary\n",
    "                # do nothing\n",
    "                pass\n",
    "\n",
    "        pass\n",
    "    return(dist_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Single process method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# co_oc1 = create_dist_map(1)\n",
    "# co_oc2 = create_dist_map(2)\n",
    "# co_oc3 = create_dist_map(3)\n",
    "# co_oc4 = create_dist_map(4)\n",
    "# co_oc5 = create_dist_map(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multi-processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20142184/20142184 [1:34:38<00:00, 3546.79it/s]  \n",
      " 90%|████████▉ | 18118721/20142184 [1:39:29<10:13, 3299.26it/s]\n",
      "100%|██████████| 20142184/20142184 [1:44:30<00:00, 3212.00it/s]\n",
      "100%|██████████| 20142184/20142184 [1:48:25<00:00, 3096.03it/s]\n",
      "100%|██████████| 20142184/20142184 [1:51:41<00:00, 3005.82it/s]\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes=NUM_PROCESSES)  \n",
    "co_oc1, co_oc2,co_oc3,co_oc4,co_oc5 = pool.map(create_dist_map,[1,2,3,4,5])\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse.save_npz(\"co_oc_matrices/co_oc_1\",co_oc1.tocoo())\n",
    "sparse.save_npz(\"co_oc_matrices/co_oc_2\",co_oc2.tocoo())\n",
    "sparse.save_npz(\"co_oc_matrices/co_oc_3\",co_oc3.tocoo())\n",
    "sparse.save_npz(\"co_oc_matrices/co_oc_4\",co_oc4.tocoo())\n",
    "sparse.save_npz(\"co_oc_matrices/co_oc_5\",co_oc5.tocoo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load previously saved matrices\n",
    "- to compare is saved and loaded matrices are the same do \n",
    "\n",
    " ``` (saved_sparse_mat != loaded_sparse_matrix)``` \n",
    " \n",
    " and check for elements present in the newly generated sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# co_oc1 = sparse.load_npz(\"co_oc_matrices/co_oc1.npz\")\n",
    "# co_oc2 = sparse.load_npz(\"co_oc_matrices/co_oc2.npz\")\n",
    "# co_oc3 = sparse.load_npz(\"co_oc_matrices/co_oc3.npz\")\n",
    "# co_oc4 = sparse.load_npz(\"co_oc_matrices/co_oc4.npz\")\n",
    "# co_oc5 = sparse.load_npz(\"co_oc_matrices/co_oc5.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find compund tokens in any given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(test):\n",
    "    words = test.lower().split()\n",
    "    words = [i for i in words if i in word2index.keys()]\n",
    "    \n",
    "    l0 = [ 0.0 for i in words]\n",
    "    l1 = [ co_oc1[word2index[v],word2index[words[i+1]]] for i,v in enumerate(words[:-1])]\n",
    "    l2 = [ co_oc2[word2index[v],word2index[words[i+2]]] for i,v in enumerate(words[:-2])]\n",
    "    l3 = [ co_oc3[word2index[v],word2index[words[i+3]]] for i,v in enumerate(words[:-3])]\n",
    "    l4 = [ co_oc4[word2index[v],word2index[words[i+4]]] for i,v in enumerate(words[:-4])]\n",
    "    l5 = [ co_oc5[word2index[v],word2index[words[i+5]]] for i,v in enumerate(words[:-5])]\n",
    "    \n",
    "\n",
    "\n",
    "    row = \"\" \n",
    "    for i in words:\n",
    "        row += i+\"\\t\"\n",
    "    row += \"\\n\"\n",
    "    for i in l0:\n",
    "        row += str(i)+\"\\t\\t\"\n",
    "    row += \"\\n\\t\"\n",
    "    for i in l1:\n",
    "        row += str(i)+\"\\t\\t\"\n",
    "    row += \"\\n\\t\\t\"\n",
    "    for i in l2:\n",
    "        row += str(i)+\"\\t\\t\"\n",
    "    row += \"\\n\\t\\t\\t\"\n",
    "    for i in l3:\n",
    "        row += str(i)+\"\\t\\t\"\n",
    "    row += \"\\n\\t\\t\\t\\t\"\n",
    "    for i in l4:\n",
    "        row += str(i)+\"\\t\\t\"\n",
    "    row += \"\\n\\t\\t\\t\\t\\t\"\n",
    "    for i in l5:\n",
    "        row += str(i)+\"\\t\\t\"\n",
    "    row += \"\\n\"\n",
    "    print(row)\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\nSuggestions : \")\n",
    "    for ind,val in enumerate(l1):\n",
    "        if(l0[ind]<val and l0[ind+1]<val):\n",
    "            if(ind==0):\n",
    "                if(l2[ind]<val):\n",
    "                    print(\"\\t{} {} - {}\".format(words[ind],words[ind+1],val))\n",
    "            elif(ind==len(l1)-1):\n",
    "                if(l2[ind-1]<val):\n",
    "                    print(\"\\t{} {} - {}\".format(words[ind],words[ind+1],val))\n",
    "            else:\n",
    "                if(l2[ind-1]<val and l2[ind]<val):\n",
    "                    print(\"\\t{} {} - {}\".format(words[ind],words[ind+1],val))\n",
    "    \n",
    "    \n",
    "    for ind,val in enumerate(l2):\n",
    "        if(l1[ind]<val and l1[ind+1]<val):\n",
    "            if(ind == 0):\n",
    "                if(l3[ind]<val):\n",
    "                    print(\"\\t{} {} {} - {}\".format(words[ind],words[ind+1],words[ind+2],val))\n",
    "            elif(ind==len(l2)-1):\n",
    "                if(l3[ind-1]<val):\n",
    "                    print(\"\\t{} {} {} - {}\".format(words[ind],words[ind+1],words[ind+2],val))\n",
    "            else:\n",
    "                if(l3[ind-1]<val and l3[ind]<val):\n",
    "                    print(\"\\t{} {} {} - {}\".format(words[ind],words[ind+1],words[ind+2],val))\n",
    "               \n",
    "                    \n",
    "    for ind,val in enumerate(l3):\n",
    "        if(l2[ind]<val and l2[ind+1]<val):\n",
    "            if(ind == 0):\n",
    "                if(l4[ind]<val):\n",
    "                    print(\"\\t{} {} {} {} - {}\".format(words[ind],words[ind+1],words[ind+2],words[ind+3],val))\n",
    "            elif(ind==len(l3)-1):\n",
    "                if(l4[ind-1]<val):\n",
    "                    print(\"\\t{} {} {} {} - {}\".format(words[ind],words[ind+1],words[ind+2],words[ind+3],val))                \n",
    "            else:\n",
    "                if(l4[ind-1]<val and l4[ind]<val):\n",
    "                    print(\"\\t{} {} {} {} - {}\".format(words[ind],words[ind+1],words[ind+2],words[ind+3],val))\n",
    "                    pass\n",
    "                    \n",
    "    for ind,val in enumerate(l4):\n",
    "        if(l3[ind]<val and l3[ind+1]<val):\n",
    "            if(ind == 0):\n",
    "                if(l5[ind]<val):\n",
    "                    print(\"\\t{} {} {} {} {} - {}\".format(words[ind],words[ind+1],words[ind+2],words[ind+3],words[ind+4],val))\n",
    "                    pass\n",
    "            elif(ind==len(l4)-1):\n",
    "                if(l5[ind-1]<val):\n",
    "                    print(\"\\t{} {} {} {} {} - {}\".format(words[ind],words[ind+1],words[ind+2],words[ind+3],words[ind+4],val))\n",
    "                    pass                \n",
    "            else:\n",
    "                if(l5[ind-1]<val and l5[ind]<val):\n",
    "                    print(\"\\t{} {} {} {} {} - {}\".format(words[ind],words[ind+1],words[ind+2],words[ind+3],words[ind+4],val))\n",
    "                    pass\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new york stock exchange is a busy place \n",
      "\n",
      "new\tyork\tstock\texchange\tis\ta\tbusy\tplace\t\n",
      "0.0\t\t0.0\t\t0.0\t\t0.0\t\t0.0\t\t0.0\t\t0.0\t\t0.0\t\t\n",
      "\t236308\t\t2754\t\t6144\t\t277\t\t275157\t\t2626\t\t16\t\t\n",
      "\t\t2806\t\t5788\t\t703\t\t1239\t\t382\t\t6708\t\t\n",
      "\t\t\t5848\t\t840\t\t872\t\t0\t\t3853\t\t\n",
      "\t\t\t\t5436\t\t4100\t\t0\t\t22\t\t\n",
      "\t\t\t\t\t15977\t\t6\t\t28\t\t\n",
      "\n",
      "\n",
      "\n",
      "Suggestions : \n",
      "\tnew york - 236308\n",
      "\tstock exchange - 6144\n",
      "\tis a - 275157\n",
      "\ta busy place - 6708\n",
      "\tnew york stock exchange - 5848\n"
     ]
    }
   ],
   "source": [
    "s = \"new york stock exchange is a busy place \"\n",
    "print(s+\"\\n\")\n",
    "inference(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Africa is one token\n",
      "\n",
      "south\t\tafrica\t\tis\t\tone\t\ttoken\t\t\n",
      "0.0\t\t0.0\t\t0.0\t\t0.0\t\t0.0\t\t\n",
      "\t16986\t\t827\t\t36105\t\t2\t\t\n",
      "\t\t1557\t\t109\t\t6\t\t\n",
      "\t\t\t374\t\t0\t\t\n",
      "\t\t\t\t0\t\t\n",
      "\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "Suggestions : \n",
      "\tsouth africa - 16986\n",
      "\tis one - 36105\n"
     ]
    }
   ],
   "source": [
    "s = \"South Africa is one token\"\n",
    "print(s+\"\\n\")\n",
    "inference(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
